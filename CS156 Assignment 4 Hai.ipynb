{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from class to preprocess the image data\n",
    "import os\n",
    "os.chdir('D:\\/Downloads-D')\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "\n",
    "#Create a function that takes in the path for images and outputs the flattened X inputs\n",
    "def flatten(path):\n",
    "    '''\n",
    "    input: path - a string which is the path to the images \n",
    "    '''\n",
    "    # create paths for all images\n",
    "    man_images = glob(path)\n",
    "\n",
    "    man_flattened = []\n",
    "    # for each image path\n",
    "    for path in man_images:\n",
    "        # open it as a read file in binary mode\n",
    "        with open(path, 'r+b') as f:\n",
    "            # open it as an image\n",
    "            with Image.open(f) as image:\n",
    "                # resize the image to be more manageable\n",
    "                cover = resizeimage.resize_cover(image, [20, 20]) #convert to size 20,20\n",
    "                # flatten the matrix to an array and append it to all flattened images\n",
    "                man_flattened.append((np.array(cover).flatten(), 0))\n",
    "\n",
    "\n",
    "    # Flatten it once more\n",
    "    man_flattened = np.asarray(man_flattened)\n",
    "\n",
    "    # Declare which are the X and Y inputs\n",
    "    X = man_flattened[:,0]\n",
    "    Y = man_flattened[:,1]\n",
    "    X = np.delete(X, [987]) #The jersey dataset for some reason has a faulty image, so I delete it\n",
    "    Y = np.delete(Y, [987]) #Should not affect much as the data set is quite large\n",
    "    # Use np.stack to put the data into the right dimension\n",
    "    X = np.stack(i for i in X)\n",
    "    Y = np.stack(i for i in Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads-D\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "D:\\Downloads-D\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3343: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "X, y=flatten('jerseys/*')\n",
    "X2, y2 = flatten('woman_200/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output y (the labels) are now all zeroes due to the configuration of the above function. We will now change the outputs for \"woman\" to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y2)):\n",
    "    y2[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we move to concatenate the two datasets into one dataset for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = np.concatenate((X, X2))\n",
    "y_full = np.concatenate((y, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 1200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the shape of the data like above, we see that we have correctly processed the data: we have 2600 pictures, and thus we have 2600 labels (y). For x, our dimensions are 20 in width and length for each image, and then 3 more for the colors, so we have $20*20*3 = 1200$ in total.\n",
    "\n",
    "Now, we split the data into the training test (80%) and the test set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=2512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we scale the input data to help with convergence of the models we will use in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit(X_train).transform(X_train)\n",
    "X_test = StandardScaler().fit(X_test).transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data ready to train the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear classifier: Logistic Regression ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we choose the Logistic Regression model as our linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression(max_iter = 2000)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression, training set, is 0.9004807692307693\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of logistic regression, training set, is\", logistic_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression, test set, is 0.6115384615384616\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of logistic regression, test set, is\",logistic_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic regression with PCA ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we implement PCA. We test with different numbers of components to find the optimal number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained_list=[]\n",
    "x_axis=[]\n",
    "for i in range(100):\n",
    "    x_axis.append(i)\n",
    "    pca = decomposition.PCA(n_components=i)\n",
    "    pca.fit(X_train)\n",
    "    var_explained_list.append(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_axis, var_explained_list)\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we see that there will be significant diminishing utility (one extra component explain a diminishing value of extra variance) after around 10 components. Therefore, we will choose n_components = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct PCA on the data\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "#transforming the data\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conduct logistic regression on the PCA-ed data\n",
    "logistic_model_pca = LogisticRegression(max_iter = 1000)\n",
    "logistic_model_pca.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of PCA-ed logistic regression, training set, is 0.6317307692307692\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of PCA-ed logistic regression, training set, is\",logistic_model_pca.score(X_train_transformed, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of PCA-ed logistic regression, test set, is 0.6192307692307693\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of PCA-ed logistic regression, test set, is\",logistic_model_pca.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression with LDA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conducting LDA on the data\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranforming the data\n",
    "X_train_lda = lda.transform(X_train)\n",
    "X_test_lda = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conduct logistic regression on the LDA-ed data\n",
    "logistic_model_lda = LogisticRegression(max_iter = 1000)\n",
    "logistic_model_lda.fit(X_train_lda, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA-ed logistic regression, training set, is 0.9177884615384615\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of LDA-ed logistic regression, training set, is\", logistic_model_lda.score(X_train_lda, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA-ed logistic regression, test set, is 0.6076923076923076\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of LDA-ed logistic regression, test set, is\", logistic_model_lda.score(X_test_lda, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above implementation, we have the following results:\n",
    "\n",
    "a. Logistic Regression:\n",
    "\n",
    "Train accuracy: 0.90\n",
    "\n",
    "Test accuracy: 0.61\n",
    "\n",
    "b. Logistic Regression with PCA: \n",
    "\n",
    "Train accuracy: 0.63\n",
    "\n",
    "Test accuracy: 0.62\n",
    "\n",
    "c. Logistic Regression with LDA:\n",
    "\n",
    "Train accuracy: 0.92\n",
    "\n",
    "Test accuracy: 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "By definition, Logistic Regression is a linear, supervised learning model that determines if a data point, with its given features (inputs) has a higher or lower probability of being in either class (threshold is often 0.5). Meanwhile, the PCA and the LDA are both methods to generate an alternative representation of the data through dimensionality reduction. However, the difference is that LDA takes the labels into account (it is a supervised learning method), while the PCA does not (it is an unsupervised learning method). The PCA also significantly simplifies the data (we reduced to only the top 10 components above).\n",
    "\n",
    "Since supervised learning models are trained using labels, they often give a more accurate results on the training set. However, unsupervised learning models may yield more generalizable results - that is, the accuracy of the test set and the training set is not too different. \n",
    "\n",
    "This is shown through the result - using PCA decreases the accuracy of the training set, as it does not use the labels. The remaining two models, logistic regression and logistic regression with LDA, does significantly better in the training set thanks to using the labels to train. However, we see that there is overfitting in those two models - when it comes to the test set, all three methods have quite similar results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
